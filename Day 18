DESCRIPTIVE STATISTICS

A large number of methods collectively compute descriptive statistics and other related operations on DataFrame. Most of these are
aggregations like sum(), mean(), but some of them, like sumsum(), produce an object of the same size. Generally speaking, these
methods take an axis argument, just like ndarray.{sum, std, ...}, but the axis can be specified by name or integer

DataFrame − “index” (axis=0, default), “columns” (axis=1)

Live Demo
import pandas as pd
import numpy as np

#Create a Dictionary of series
d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Smith','Jack',
   'Lee','David','Gasper','Betina','Andres']),
   'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]),
   'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])
}

#Create a DataFrame
df = pd.DataFrame(d)
print df

OUTPUT-
Age  Name   Rating
0   25   Tom     4.23
1   26   James   3.24
2   25   Ricky   3.98
3   23   Vin     2.56
4   30   Steve   3.20
5   29   Smith   4.60
6   23   Jack    3.80
7   34   Lee     3.78
8   40   David   2.98
9   30   Gasper  4.80
10  51   Betina  4.10
11  46   Andres  3.65

-----------------------------
sum()
Returns the sum of the values for the requested axis. By default, axis is index (axis=0).


import pandas as pd
import numpy as np
 
#Create a Dictionary of series
d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Smith','Jack',
   'Lee','David','Gasper','Betina','Andres']),
   'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]),
   'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])
}

#Create a DataFrame
df = pd.DataFrame(d)
print df.sum()

Its output is as follows −

Age                                                    382
Name     TomJamesRickyVinSteveSmithJackLeeDavidGasperBe...
Rating                                               44.92
dtype: object
--------------------------------------------------------------
CENTRALITY MEASURES, DISPERSION MEASURES WITH NUMPY AND SCIPY MODULES-

Centrality measures in numpy-
import networkx as nx 

def degree_centrality(G, nodes): 
	r"""Compute the degree centrality for nodes in a bipartite network. 

	The degree centrality for a node `v` is the fraction of nodes 
	connected to it. 

	Parameters 
	---------- 
	G : graph 
	A bipartite network 

	nodes : list or container 
	Container with all nodes in one bipartite node set. 

	Returns 
	------- 
	centrality : dictionary 
	Dictionary keyed by node with bipartite degree centrality as the value. 

	Notes 
	----- 
	The nodes input parameter must contain all nodes in one bipartite node set, 
	but the dictionary returned contains all nodes from both bipartite node 
	sets. 

	For unipartite networks, the degree centrality values are 
	normalized by dividing by the maximum possible degree (which is 
	`n-1` where `n` is the number of nodes in G). 

	In the bipartite case, the maximum possible degree of a node in a 
	bipartite node set is the number of nodes in the opposite node set 
	[1]_. The degree centrality for a node `v` in the bipartite 
	sets `U` with `n` nodes and `V` with `m` nodes is 

	.. math:: 

		d_{v} = \frac{deg(v)}{m}, \mbox{for} v \in U , 

		d_{v} = \frac{deg(v)}{n}, \mbox{for} v \in V , 


	where `deg(v)` is the degree of node `v`.		 

	
	"""
	top = set(nodes) 
	bottom = set(G) - top 
	s = 1.0/len(bottom) 
	centrality = dict((n,d*s) for n,d in G.degree_iter(top)) 
	s = 1.0/len(top) 
	centrality.update(dict((n,d*s) for n,d in G.degree_iter(bottom))) 
	return centrality 


The above function is invoked using the networkx library and once the library is installed, you can eventually use it and the following code has to be written in python for the implementation of the Degree centrality of a node.


import networkx as nx 
G=nx.erdos_renyi_graph(100,0.5) 
d=nx.degree_centrality(G) 
print(d) 

output-
{0: 0.5252525252525253, 1: 0.4444444444444445, 2: 0.5454545454545455, 3: 0.36363636363636365, 
4: 0.42424242424242425, 5: 0.494949494949495, 6: 0.5454545454545455, 7: 0.494949494949495, 
8: 0.5555555555555556, 9: 0.5151515151515152, 10: 0.5454545454545455, 11: 0.5151515151515152, 
12: 0.494949494949495, 13: 0.4444444444444445, 14: 0.494949494949495, 15: 0.4141414141414142, 
16: 0.43434343434343436, 17: 0.5555555555555556, 18: 0.494949494949495, 19: 0.5151515151515152, 
20: 0.42424242424242425, 21: 0.494949494949495, 22: 0.5555555555555556, 23: 0.5151515151515152, 
24: 0.4646464646464647, 25: 0.4747474747474748, 26: 0.4747474747474748, 27: 0.494949494949495, 
28: 0.5656565656565657, 29: 0.5353535353535354, 30: 0.4747474747474748, 31: 0.494949494949495, 
32: 0.43434343434343436, 33: 0.4444444444444445, 34: 0.5151515151515152, 35: 0.48484848484848486, 
36: 0.43434343434343436, 37: 0.4040404040404041, 38: 0.5656565656565657, 39: 0.5656565656565657, 
40: 0.494949494949495, 41: 0.5252525252525253, 42: 0.4545454545454546, 43: 0.42424242424242425, 
44: 0.494949494949495, 45: 0.595959595959596, 46: 0.5454545454545455, 47: 0.5050505050505051, 
48: 0.4646464646464647, 49: 0.48484848484848486, 50: 0.5353535353535354, 51: 0.5454545454545455, 
52: 0.5252525252525253, 53: 0.5252525252525253, 54: 0.5353535353535354, 55: 0.6464646464646465, 
56: 0.4444444444444445, 57: 0.48484848484848486, 58: 0.5353535353535354, 59: 0.494949494949495, 
60: 0.4646464646464647, 61: 0.5858585858585859, 62: 0.494949494949495, 63: 0.48484848484848486, 
64: 0.4444444444444445, 65: 0.6262626262626263, 66: 0.5151515151515152, 67: 0.4444444444444445, 
68: 0.4747474747474748, 69: 0.5454545454545455, 70: 0.48484848484848486, 71: 0.5050505050505051, 
72: 0.4646464646464647, 73: 0.4646464646464647, 74: 0.5454545454545455, 75: 0.4444444444444445, 
76: 0.42424242424242425, 77: 0.4545454545454546, 78: 0.494949494949495, 79: 0.494949494949495, 
80: 0.4444444444444445, 81: 0.48484848484848486, 82: 0.48484848484848486, 83: 0.5151515151515152, 
84: 0.494949494949495, 85: 0.5151515151515152, 86: 0.5252525252525253, 87: 0.4545454545454546, 
88: 0.5252525252525253, 89: 0.5353535353535354, 90: 0.5252525252525253, 91: 0.4646464646464647, 
92: 0.4646464646464647, 93: 0.5555555555555556, 94: 0.5656565656565657, 95: 0.4646464646464647, 
96: 0.494949494949495, 97: 0.494949494949495, 98: 0.5050505050505051, 99: 0.5050505050505051} 

------------------------------------------------------
DEGREE CENTRALITY-

def answer_two():
    """returns the node with the best degree centrality"""
    return largest_node(DEGREE_CENTRALITY)
print(answer_two())
105
--------------------------------------
DISPERSION MEASURES-
var_UnitPrice=Retail['UnitPrice'].var()
var_UnitPrice

9362.469164424467

--------------
std_UnitPrice=Retail['UnitPrice'].std()
std_UnitPrice 

96.75985306119716
----------------------
var_quantity=Retail['Quantity'].var()
var_quantity

47559.39140913822
----------------------------------------------------
RANDOM DISTRIBUTION-
 mu, sigma = 0, 0.1 # mean and standard deviation
>>> s = np.random.normal(mu, sigma, 1000)

Verify the mean and the variance:
>>>
>>> abs(mu - np.mean(s)) < 0.01
True
>>>
>>> abs(sigma - np.std(s, ddof=1)) < 0.01
True
---------------------------------------

HYPOTHESIS TESTING BY SCIPY

from scipy import stats
>>>stats.ttest_1samp(data['VIQ'], 0)   
Ttest_SampleResult(statistics=30.08880....,pvalue  =1.3289....e-28)
 
---------------------------------------------------------------------
